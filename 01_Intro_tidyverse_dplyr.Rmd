---
title: "Introduction to tidyverse - A new grammar"
author: "Francesco Maria Sabatini"
date: "3/22/2020"
output: html_document
---
  
**Timestamp:** `r date()`  
**Drafted:** Francesco Maria Sabatini  
**Revised:**  
**version:** 1.0

`tidyverse` is a a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Working in the `tidyverse` requires a basic understanding of `dplyr`, 
a package which provides a set of tools for efficiently manipulating datasets in R.  
What I personally love is that `dplyr` is **synthetic, fast and easy to read**. It allows to write code which is easy to explain, revise and mantain. Knowing that whather analysis we do, we will probably have to adjust and re-run it over and over, using `dplyr`'s grammar now is the best present we can give to our *future selves*, since it will help us save a *lot* of time later.  
\newline 

The overarching goals of this tutorial is to fall in love with `dplyr` and to see some cute wild cats. The specific goals are:  
- Understand the logic of `dplyr` and the power of piping `%>%`  
- Explore the key verbs of `dplyr`'s grammar  
- Simulate a `dplyr` based workflow  
\newline

You will learn how to use the key verbs of `dplyr`, including `select()`, `filter()`, `mutate()`, `summarize()`, `rename()`, as well as their generalized `x_at()` and `x_all()` versions. We'll also touch on the `join` family of functions.  

\newline

We will first create a simulated dataframe using data from `GBIF` and environmental predictors from some global datasets.

```{r results="hide", message=F, warning=F}
library(tidyverse)
library(downloader)
library(rgbif)

## Markdown packages
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
## Spatial packages
#install.packages(c("rgdal", "sp", "sf", "rgeos", "raster", "rnaturalearth"))
library(rgdal)
library(sp)
library(sf)
library(rgeos)
library(raster)
library(rnaturalearth)

#save temporary files
write("TMPDIR = _tmp", file=file.path(Sys.getenv('TMPDIR'), '.Renviron'))
write("R_USER = _tmp", file=file.path(Sys.getenv('R_USER'), '.Renviron'))
rasterOptions(tmpdir="_tmp")
```

### Import species data from `GBIF`
As a toy dataset, we will compare the climatic niche of the five cutest wild cats out there. The selection follows the authoritative source [www.backyardcatenclosures.com.au](https://www.backyardcatenclosures.com.au/blogs/cute-cat/5-cute-wild-cat-breeds).  
\newline


Let's get familiar with our cute cats. We first create a data.frame of names, short names and urls to retrieve pictures of our cute cats:
```{r}
myspecies <- c("Felis manul", #Palla's cat
               "Caracal caracal", #Caracal
               "Felis margarita", #Sand cat
               "Prionailurus rubiginosus", #Rusty spotted cat
               "Leopardus wiedii" #Margay
               )
shortnames <- c("Palla's cat", "Caracal", "Sand Cat", "Rusty Spotted Cat", "Margay")
urls <- c("https://upload.wikimedia.org/wikipedia/commons/d/d6/Manoel.jpg", #pallas
          "https://upload.wikimedia.org/wikipedia/commons/a/a3/Caracl_%2801%29%2C_Paris%2C_d%C3%A9cembre_2013.jpg", #caracal
          "https://upload.wikimedia.org/wikipedia/commons/e/e9/Persian_sand_CAT.jpg", #Sand cat
          "https://upload.wikimedia.org/wikipedia/commons/3/3e/Rusty_spotted_cat_1.jpg", #Rusty spotted cat
          "https://upload.wikimedia.org/wikipedia/commons/b/bd/Margay_in_Costa_Rica.jpg")

cute <- data.frame(species=myspecies, 
                   short=shortnames, 
                   url=urls)
```




```{r, message=F, echo=F, warning=F}
library(cowplot)
library(magick)

Pallas <- ggdraw() + draw_image(urls[1], scale = 0.9) + 
  draw_figure_label(label = "Palla's cat")
Caracal <- ggdraw() + draw_image(urls[2], scale = 0.9) + 
  draw_figure_label(label = "Caracal")
Sand <- ggdraw() + draw_image(urls[3], scale = 0.9) + 
  draw_figure_label(label = "Sand cat")
Rusty <- ggdraw() + draw_image(urls[4], scale = 0.9) + 
  draw_figure_label(label = "Rusty Spotted cat")
Margay <- ggdraw() + draw_image(urls[5], scale = 0.7) + 
  draw_figure_label(label = "Margay")

plot_grid( 
  plot_grid(Pallas, Caracal, nrow=1, align="v"), 
  plot_grid(Sand, Rusty, Margay, nrow=1, rel_heights=c(1,1, 0.5)), 
  nrow=2, rel_heights = c(1, 1.5))
```
\newline 

*All pictures are from https://en.wikipedia.org/*
\newline  

Download occurrence data from `GBIF`

```{r}
get.speciesKey <- function(x){name_backbone(x)$speciesKey} #get GBIF species key
key <- unlist(lapply(myspecies, get.speciesKey))
get.100occurrences <- function(x){occ_search(taxonKey=x, return="data", limit=100)}
#dat <- occ_search(taxonKey=key, return='data', limit=300)
dat <- lapply(key, get.100occurrences)
```


We produce a list of 5 elements, each with up to 100 occurrences for a cat species. Let's take a look at the output:

```{r, echo=F}
knitr::kable(dat[[1]][1:10,], 
             caption="Five cutest wild cat species from GBIF (first cat, first 10 rows shown)", digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                full_width = F, position = "center")
```

It's pretty horrible, and there's a lot of information we probably do not need. 
First, notice that each element of the list is a `tibble`, which is the `dplyr` equivalent of a `data.frame`. It has some cool tweaks (try to visualize a tibble in your console) compared to a data.frame. You  should be aware, though, that some functions designed for data.frames may not work with tibbles. Try converting back and forth from data.frame to tbl with the commands `as.data.frame()` and `as.tbl()`.  
\newline

### Clean data
Let's see how `dplyr` can help us to clean up our data into something we can work with:   
1) bind all rows into a single data.frame  
2) select the columns we need (species, country, coordinates)  
3) filter out all observation without spatial coordinates  
4) rename columns to make our life easier later  
\newline

We'll do this through piping  

The command pipe `%>%` allows to use the output of a previous function as the input of the following. This means we can avoid a *lot* redundancy in our code. (Do yourself a favour and memorize the RStudio shortcut: `Ctrl + Shift + m`)  
\newline

Let's compare the code in `base` vs. the code in `dplyr`.

```{r}
#base
#Since each element of the list has a different number of columns, I cannot simply bind them by rows, but I first need to subset them to the same format.
dat_clean <- NULL
for(i in 1:5){
  dat_clean <- rbind(dat_clean, 
                     dat[[i]][,which(colnames(dat[[i]]) %in% 
                                       c("species", "country", "decimalLongitude", "decimalLatitude"))])
  }
dat_clean <- dat_clean[which(!is.na(dat_clean$decimalLatitude) & 
                               !is.na(dat_clean$decimalLongitude)),]
colnames(dat_clean)[2:3] <- c("Lat", "Lon")
dat_clean <- dat_clean[,c(1,4,3,2)] #reorder columns
head(dat_clean)
```

```{r}
#dplyr
dat_clean <- dat %>% 
  bind_rows() %>% 
  dplyr::select(species, country, decimalLongitude, decimalLatitude) %>% 
  filter(!is.na(decimalLongitude) & !is.na(decimalLatitude)) %>% 
  rename(Lon=decimalLongitude, Lat=decimalLatitude)
head(dat_clean)
```
Shorter, cleaner, easier.  
Congratulations. You did your first piping. A few features worth noting:  
- In the first row, we assign the output to the object `dat_clean` and select `dat` as initial input of our chain of commands. The object `dat` will be fed into the first function (`bind_rows()`). The output from the first function will be fed as first argument in the second function (`select()`) and so on  
- `bind_rows()` is a pretty powerful alternative to rbind, which has the desirable properties of binding rows only when these have the same column name (!), which avoids us having to select columns separately for each element of the `dat` list in a loop    
- For the function `select()` I specified the package because there's a conflict with the `raster::select()` function. When doing spatial analysis, it's more robust to consistently specifiy the package when conflicts may exist  
- `dplyr` *DOES NOT* need quoting column names (but doesn't mind if you quote either). It may be confusing at first, but it allows to work quicker, and avoids filling up your code with distracting elements.  
\newline

It can be tedious to explicitly specify all the column names to select. `dplyr` is the best friend of lazy people, and provide a quicker way for selecting multiple columns.
```{r}
dat2 <- dat %>% 
  bind_rows() %>% 
  dplyr::select(kingdom:species, year:day, country, decimalLongitude, decimalLatitude, everything())
```
See how we used the function `everything()` to select all the remaining columns (which can be really useful when we are simply changing the order of our columns)




### Create a map of the data
We are now ready to plot these points on a map. 
The map of the worls here derives from the package `rnaturalearth`. Plotting is done using `ggplot` another important member of the `tidyverse`. Spatial data are handled using the `sf` library. All very cool stuff that we don't discuss here.
```{r, message=F, warning=F}
countries <- ne_countries(returnclass = "sf") %>% 
  st_geometry()

## basic graph of the world in Eckert projection
ggplot() +
  geom_sf(data = countries, fill = "grey90", col = NA, lwd = 0.3) +
  geom_point(data=dat_clean, aes(x=Lon, y=Lat, col=species)) +
  theme_bw() 
```
\newline

There's a point for the Sand cat which is clearly out of the species native range. We delete it.
```{r}
dat_clean <- dat_clean %>% 
  filter(!(species=="Felis margarita" & country=="United States of America"))
```

### Extract predictors from global datasets
Imagine we are interesting in calculating some metrics related to the environmental niche of the five cute cat species. To do spatial operations, we need to transform our dataset into a `SpatialPointDataFrame()`. 
```{r}
dat.shp <- SpatialPointsDataFrame(coords=dat_clean %>% 
                                    dplyr::select(Lon, Lat), 
                                  data=dat_clean %>% 
                                    dplyr::select(species, country), 
                                  proj4string = CRS("+init=epsg:4326")) #Lat long in WGS 84
                                  
```
We can then download Bioclimatic variables from [CHELSA](http://chelsa-climate.org/bioclim/)  
We focus on two only:  
\newline
  
Bio1 = Annual Mean Temperature  
Bio12 = Annual Precipitation  

\newline 

\newline
Download raster files:
```{r, message=F, warning=F, eval=F}
dir.create("Ancillary_data")
dir.create("Ancillary_data/CHELSA")
url.chelsa <- list()
ii <- stringr::str_pad(c(1,12), width=2, side="left", pad="0")
url.chelsa <- paste("https://envidatrepo.wsl.ch/uploads/chelsa/chelsa_V1/bioclim/integer/CHELSA_bio10_", ii, ".tif", sep="")
for(i in 1:2){
  download(url.chelsa[[i]],
           paste("Ancillary_Data/CHELSA/CHELSA_bio10_", ii[i], ".tif", sep=""), 
           mode = "wb")
}
```

Load CHELSA rasters, and intersect `dat.shp` with each of them 
```{r, warning=F, message=F}
bio01.raster <- raster("Ancillary_Data/CHELSA/CHELSA_bio10_01.tif")
bio12.raster <- raster("Ancillary_Data/CHELSA/CHELSA_bio10_12.tif")

bio01 <- raster::extract(bio01.raster, dat.shp) # Â°C * 10
bio12 <- raster::extract(bio12.raster, dat.shp) # mm
```
Bind columns and transform temperature data (they are multiplied by 10 so that they can be stored as integers). 
```{r}
envdata <- dat_clean %>% 
  mutate(Temp=bio01) %>% 
  mutate(P=bio12) %>% 
  mutate(Temp=Temp/10)
```
The function `mutate()` is a fundamental one. It allows creating and modifying a column, and can be used to do operations between columns.  
\newline

Now things get interesting. To calculate the climatic niche we may want to calculate some statistics for each predictor for species (min, max, mean). `dplyr` comes to our help with the function `group_by()` (and its opposite `ungroup()`). Once you group a data.frame by a factor (or character!), all operations are applied on a group by group basis. Easy, quick, efficient. We can then use the function `summarize()`.
```{r}
niche <- envdata %>% 
  group_by(species) %>% 
  summarize(n=n(), 
            min.T=min(Temp, na.rm=T),
            mean.T=mean(Temp, na.rm=T),
            max.T=max(Temp, na.rm=T),
            min.P=min(P, na.rm=T),
            mean.P=mean(P, na.rm=T),
            max.P=max(P, na.rm=T))
```
Checkout how we used the function `n()` to count the number of observations for each group!.  
*TIP* Once you group a `tibble` it stays grouped! Don't forget to ungroup afterwards!  
\newline

How do our cute cats rank in terms of their resistance to maximum temperature? All we have to do is to `arrange()` our rows in a descending order.
```{r}
niche %>% 
  arrange(desc(max.T))
```
To sort in ascending order, it's enough to do `arrange(niche, max.T)`.  
\newline

You, fine ecologists, may be unhappy of the unequal sample size in our data. How would the results change if we were using only one randomly selected observations for each species in each country?  
\newline 

Easy-peasy.  
\newline 


All we need to do is to group our data by both species AND country, and then we can take advantage of a bunch of useful commands which subset our dataset by selecting only a set of rows. Check `slice()`, `sample_n()`, `sample_frac()`.
```{r}
envdata %>% 
  group_by(species, country) %>% 
  sample_n(1, replace=F) %>% 
  head()
```
`sample_n()` randomly samples n rows for each group. `slice(1:n)` can be used to subset the dataset by row numbers  
\newline 

Note that many `base` functions useful for checking the shape of your dataset work also inside pipes. Try for instance to add the commands `nrow()`, `ncol()`, `dim()`, `class()`, `str()`, `head()`, `summary()` at the end of your pipe .  
\newline

Often, you have the need to delete duplicated rows in your dataset. Let's say we want to count the number of countries we have data for, separately for each cute cat. Our best friend is the command `distinct()`
```{r}
ncountries <- envdata %>% 
  group_by(species) %>% 
  distinct(country) %>% 
  summarize(n=n())
```
!! *watch out* !! By using `distinct` you effectively use (and retain) only the information in the column(s) you are using `distinct` on. If you want to retain the information in the other columns, try `distinct(country, .keep_all=T)`.  
\newline


Transforming all your scripts to `dplyr` grammar may be challenging at first. Sometimes you want to extract one column from a dataframe and use it as a vector. You can obviously run your pipe, and then subset the output using the `$` sign, as usual. A real `dplyr` feticist would never do it, though, but rather use `pull()`.  
Let's say we want to extract the string of countries we have data for (altogehter, this time, not separately by species) 
```{r}
envdata %>% 
  distinct(country) %>% 
  pull(country)
```
Cool. This is it. Now you know the basics and you can start navigating on your own.
\newline 

### Your turn (1)
Ready to answer some cute-cat related questions based on our data?  
\newline

**1) What cute cat has the widest range in term of precipitation?**  
**2) Which countries host the highest number of different species of cats?**  
**3) Which species occur in the countries from Q2 (pick up one)?**  
**3b) BONUS POINT - Can you extract the species for all countries from Q2 within the same pipeline?**  
\newline

OBVIOUSLY - you can only answer these questions using the `dplyr` functions showed so far in *ONE* single pipeline. Data in, data out. You find the solutions at the end of the document.



### Slighlty more advanced stuff
The power of `dplyr` is that you can really make your data management workflow more general, by writing your code more programmatically. We are only touching this argument here, but I want to introduce you to two cool families of functions.

#### *Joins*  
Often, we need to join information based on two or more data.frames, based on a common field. `dplyr` has the whole family of join functions which we can use: `left_join()`, `right_join()`, `full_join()`, `inner_join()`. Here I just show one simple application.  
Let's say we want to standardize the Temperature values of each individual observation, by the maximum Temperature we calculated for each cute cat species. Basically, we need to join our `envdata` object with our `niche` object, using `species` as key.
```{r}
envdata.st <- envdata %>% 
  left_join(niche %>% 
              dplyr::select(species, max.T), 
            by="species") %>% 
  mutate(Temp.st=Temp/max.T)
head(envdata.st)
```
Notice how we nested a pipe within a pipe!

Not everybody knows, you can join an object with itself using the point `.`. In `dplyr` grammar the point `.` sign symbolizes 'itself' and its use is widespread. In the next example we first calculate the maximum temperature, species by species, and then join these summarized values to the `envdata` object

```{r}
envdata.st <- envdata %>%
  left_join(x=., 
            y={.} %>% 
              group_by(species) %>% 
              summarize(max.T=max(Temp, na.rm=T)), 
            by="species") %>% 
  mutate(Temp.st=Temp/max.T)
head(envdata.st)
```
Not convinced by the power? Let's see how to do the same in `base`
```{r}
#calculate max for each species
Tmax.sp <- tapply(envdata$Temp, envdata$species, "max", na.rm=T)
Tmax.sp <- data.frame(species=names(Tmax.sp), max.T=Tmax.sp)
index1 <- match(envdata$species, Tmax.sp$species)
envdata2 <- data.frame(envdata, max.T=Tmax.sp$max.T[index1])
envdata2$Temp.st <- envdata2$Temp/envdata2$max.T
head(envdata2)
```
Three versions for the same task. Which one is easier to read?  
\newline

#### *`_at` & `_all` family*  
All basic functions in `dplyr` have an `x_at()` and an `x_all()` version. This helps **enormously** to script in a more general and programmatical fashion.  
Remember when we summarized temperature and precipitation to get min, mean and max? Actually, our code had some redundancy that a `dplyr` would never accept, since we needed to specify the same functions for each of the variables we wanted to calculate those functions on. Imagine we have to calculate summaries for all the 18 bioclim variables in Chelsa. Should we repeat the functions we want 18 times?  
NO!!  
We can simply use the function `summarize_at()`. The grammar is slighlty more complicated, but discloses a world of opportunities.
```{r}
niche <- envdata %>% 
  group_by(species) %>% 
  summarize_at(.vars=vars(Temp, P), 
               .funs=list(min=~min(., na.rm=T), 
                          mean=~mean(., na.rm=T), 
                          max=~max(., na.rm=T)))
niche
```
Awesome! Not only we summarized over both variables, but we also got columns having the correct naming. HOW COOL IS THAT?  
Spend a minute looking at the syntax of `summarize_at()`. Not exactly simple, with those point `.` and tilde `~` signs. Definitely worth learning, though.  
\newline

If it's not yet enough, let's look at another application of the `_at` family.  
Let's say we are doing an analysis on the summarized data, and decided to transform to log all the summarized variables referring to P in `niche`.

```{r}
niche.log <- niche %>% 
  mutate_at(.vars=vars(starts_with("P_")), 
            .funs=list(~log(.)))
niche.log
```
Notice that, not defining new names in the `.funs` argument, we are not overwriting the variables that we selected in the `.vars` argument. The function `starts_with()` (and its companions `ends_with()`, `contain()`), are cool helper functions specifically thought for selecting columns (which means they can also be used when using the `select()` function).  
\newline

### Your turn (2)
**4) Attach to the `envdata` object both the short names and picture urls from the object `cute`. Find the cat with the second highest number of occurrences, use the corresponding url to draw its picture, and the short name for the label (see code for plotting)**  
**5) How can you use the function `rename_all` to add the prefix "cute" to all column names in `envdata`?**  
\newline

Q4-hint - Check what happens when doing `left_join` first and then `summarize`, or the other way around. Is the result the same?  
Q5-hint - Take a look at the `summarize_at` example above. If you rename *all* fields, do you really need to specify the argument .vars?  
```{r, eval=F}
ggdraw() + draw_image("myurl") + draw_figure_label("mylabel")
```




Enough for today. We only scratched the surface here, but I'm sure you saw the huge potential for writing quicker and cleaner code compared to `base R`. Now you know the power of `dplyr`. Will you be able to harness it?

\newpage
### Solutions
```{r}
#Question 1
niche %>% 
  mutate(range.P=P_max-P_min) %>% 
  arrange(desc(range.P)) %>% 
  slice(1) %>% 
  pull(species)
#Question 2
topcountries <- dat_clean %>% 
  group_by(country) %>% 
  distinct(species) %>% 
  summarize(n=n()) %>% 
  arrange(desc(n)) %>% 
  filter(n>1) %>% 
  pull(country)
topcountries
#Question3
dat_clean %>% 
  filter(country == topcountries[1]) %>% 
  distinct(species) %>%
  pull(species)
#Questio3b
dat_clean %>% 
  filter(country %in% topcountries) %>% 
  distinct(country, species) %>% 
  arrange(country)
```

```{r}
#Question 4 
second.most.common <- envdata %>% 
  group_by(species) %>% 
  summarize(n=n()) %>% 
  left_join(cute, by="species") %>% 
  mutate(url=as.character(url)) %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  slice(2)

ggdraw() + 
  draw_image(second.most.common %>% pull(url), scale=0.5) + 
  draw_figure_label(second.most.common %>% pull(short))

#Question 5
envdata %>% 
  rename_all(.funs=~paste("cute", ., sep="_"))
```


